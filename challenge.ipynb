{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Line and Vehicle Detection and Tracking Project \n",
    "\n",
    "Line and Vehicle detection in a video stream using image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# General system level packages\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "\n",
    "# For numerical and image processing\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# For images and plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# For image processing\n",
    "from skimage.feature import hog\n",
    "# For processing video files\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# For image processing \n",
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "# For machine learning tasks such as standardization, splitting and linear SVM classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# For processing video files \n",
    "from moviepy.editor import VideoFileClip\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Import functions library for the project\n",
    "from Cha_Line import Line\n",
    "\n",
    "# Inline plotting \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Function library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##Vehicle Detection\n",
    "\n",
    "def color_hist(img, nbins = 32):\n",
    "    \"\"\"Computes the histogram of each color channel, concatenate them together\n",
    "    and return the data as features for classifier\"\"\"\n",
    "    \n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins)\n",
    "    \n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    \n",
    "    # Histogram function returns a tuple of two arrays.\n",
    "    # first element contains the counts in each of the bins\n",
    "    # second element contains the bin edges (it is one element longer than first one)\n",
    "    # Generate bin centers - we can prefer to use any channel\n",
    "    bin_edges = channel1_hist[1]\n",
    "    bin_centers = (bin_edges[1:] + bin_edges[0:len(bin_edges) - 1]) / 2\n",
    "    \n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features, bin_centers, channel1_hist, channel2_hist, channel3_hist\n",
    "\n",
    "\n",
    "def bin_spatial(img, size = (32, 32)):\n",
    "    \"\"\"Spatial binning of the image to do downsampling\"\"\"\n",
    "    features = cv2.resize(img, size, interpolation = cv2.INTER_NEAREST).ravel()\n",
    "    # Return the feature vector\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, vis = False, feature_vec = True):\n",
    "    \"\"\"Function that return HOG features\"\"\"\n",
    "    # HOG is robust to variations in shape while keeping the signature distinct enough\n",
    "    # It computes the gradient of the image and creates histograms based on the cells defined\n",
    "    # Histograms are constructed based on the weighted count of gradient directions in each cell\n",
    "    # Here weight is the magnitude of the gradien\n",
    "    \n",
    "    # The scikit-image hog() function takes in a single color channel or grayscaled image as input,\n",
    "    # as well as various other parameters.\n",
    "    # These parameters include orientations, pixels_per_cell and cells_per_block.\n",
    "    # The number of orientations is specified as an integer, and represents the number of orientation\n",
    "    #         bins that the gradient information will be split up into in the histogram.\n",
    "    #         Typical values are between 6 and 12 bins.\n",
    "    # The pixels_per_cell parameter specifies the cell size over which each gradient histogram is computed.\n",
    "    #         This paramater is passed as a 2-tuple so you could have different cell sizes in x and y,\n",
    "    #         but cells are commonlychosen to be square.\n",
    "    # The cells_per_block parameter is also passed as a 2-tuple, and specifies the local area over which the\n",
    "    #         histogram counts in a given cell will be normalized. Block normalization is not necessarily\n",
    "    #         required, but generally leads to a more robust feature set.\n",
    "    \n",
    "    # When we specify feature_vec=True then we will get an the feature vectors calculated for the specified\n",
    "    # parameters\n",
    "    \n",
    "    # Return two outputs if visualization is True\n",
    "    if vis:\n",
    "        features, hog_image = hog(img, orientations = orient,\n",
    "                                  pixels_per_cell = (pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block = (cell_per_block, cell_per_block),\n",
    "                                  transform_sqrt = False,\n",
    "                                  visualise = vis, feature_vector = feature_vec)\n",
    "        return features, hog_image\n",
    "    # Return only features when visualization is False\n",
    "    else:\n",
    "        features = hog(img, orientations = orient,\n",
    "                       pixels_per_cell = (pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block = (cell_per_block, cell_per_block),\n",
    "                       transform_sqrt = False,\n",
    "                       visualise = vis, feature_vector = feature_vec)\n",
    "        return features\n",
    "\n",
    "\n",
    "def cs_convert_from_rgb(img, color_space = 'RGB'):\n",
    "    \"\"\"Function that converts RGB image to a desired color space\"\"\"\n",
    "    # Options are HSV, LUV, HLS, YUV and YCrCb\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            new_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            new_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            new_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            new_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            new_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else:\n",
    "        new_image = np.copy(img)\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def extract_features(imgs, color_space = 'RGB', spatial_size = (32, 32), hist_bins = 32,\n",
    "                     orient = 9, pix_per_cell = 8, cell_per_block = 2, hog_channel = 0,\n",
    "                     spatial_feat = True, hist_feat = True, hog_feat = True):\n",
    "    \"\"\"Function that extract features from images using color histogram, binned spatial and HOG features\"\"\"\n",
    "    # This function gets all the parameters that are used in the individual feature extraction functions\n",
    "    # Calls color_hist(), bin_spatial(), get_hog_features() functions based on the parameters specified (True/False)\n",
    "    # Using the same ordering of the features is important for training and testing\n",
    "    # Need to provide a list of image paths and the function will loop through them\n",
    "    # Each image will have a 1-D array of features. Function will append this array to features list and return it\n",
    "    \n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    \n",
    "    # Iterate through the list of images\n",
    "    for file in imgs:\n",
    "        file_features = []\n",
    "        \n",
    "        # Read in each one by one\n",
    "        image = mpimg.imread(file)\n",
    "        \n",
    "        # apply color conversion if other than 'RGB'\n",
    "        feature_image = cs_convert_from_rgb(image, color_space)\n",
    "        \n",
    "        if spatial_feat == True:\n",
    "            # Get binned spatial features and update file_feature list\n",
    "            spatial_features = bin_spatial(feature_image, size = spatial_size)\n",
    "            file_features.append(spatial_features)\n",
    "        \n",
    "        if hist_feat == True:\n",
    "            # Get color histogram featueres and update file_feature list\n",
    "            hist_features, _, _, _, _ = color_hist(feature_image, nbins = hist_bins)\n",
    "            file_features.append(hist_features)\n",
    "        \n",
    "        if hog_feat == True:\n",
    "            # Get HOG features for desired channels and update file_feature list\n",
    "            if hog_channel == 'ALL':\n",
    "                hog_features = []\n",
    "                for channel in range(feature_image.shape[2]):\n",
    "                    hog_features.append(get_hog_features(feature_image[:,:,channel],\n",
    "                                                         orient, pix_per_cell, cell_per_block,\n",
    "                                                         vis = False, feature_vec = True))\n",
    "                hog_features = np.ravel(hog_features)\n",
    "            else:\n",
    "                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient,\n",
    "                                                pix_per_cell, cell_per_block,\n",
    "                                                vis = False, feature_vec = True)\n",
    "            file_features.append(hog_features)\n",
    "        \n",
    "        # Update main feature list by 1-D file features\n",
    "        features.append(np.concatenate(file_features))\n",
    "    \n",
    "    # Return list of feature vectors\n",
    "    return features\n",
    "\n",
    "\n",
    "def slide_window(img, x_start_stop = [None, None], y_start_stop = [None, None], xy_window = (64, 64), xy_overlap = (0.5, 0.5)):\n",
    "    \"\"\"Function that returns sliding window positions\"\"\"\n",
    "    \n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1] #columns\n",
    "    \n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0] #rows\n",
    "    \n",
    "    # Compute the span of the region to be searched\n",
    "    x_span = x_start_stop[1] - x_start_stop[0]\n",
    "    y_span = y_start_stop[1] - y_start_stop[0]\n",
    "    \n",
    "    # Compute the number of pixels per step in x/y\n",
    "    x_pixels_per_step = np.int(xy_window[0] * (1.0 - xy_overlap[0]))\n",
    "    y_pixels_per_step = np.int(xy_window[1] * (1.0 - xy_overlap[1]))\n",
    "    \n",
    "    # Compute the number of windows in x/y\n",
    "    x_window_count = np.int(x_span / x_pixels_per_step) - 1\n",
    "    y_window_count = np.int(y_span / y_pixels_per_step) - 1\n",
    "    \n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    \n",
    "    # Loop through finding x and y window positions\n",
    "    for ypos in range(y_window_count):\n",
    "        for xpos in range(x_window_count):\n",
    "            # Calculate each window position\n",
    "            window = ((xpos * x_pixels_per_step + x_start_stop[0],\n",
    "                       ypos * y_pixels_per_step + y_start_stop[0]),\n",
    "                      (xpos * x_pixels_per_step + x_start_stop[0] +\n",
    "                       xy_window[0], ypos * y_pixels_per_step + y_start_stop[0] + xy_window[1]))\n",
    "            \n",
    "            # Append window position to list\n",
    "            window_list.append(window)\n",
    "    \n",
    "    # Return the list of windows\n",
    "    return window_list\n",
    "\n",
    "\n",
    "def find_cars(img, scale, x_start_stop, y_start_stop, clf, scaler,\n",
    "              orient, pix_per_cell, cell_per_block, spatial_size, hist_bins, color_space):\n",
    "    \"\"\"Function that efficiciently loops through windows and classify them\"\"\"\n",
    "    # The benefit of this function is that you don't have to call HOG features function all the time\n",
    "    # It gets the features once at the beginning for the entire image and then subsets the related window area\n",
    "    # This way the computation time decreases significantly\n",
    "    # The scale parameters scales the image so that window size does not have to be changed\n",
    "    # NOTE: This function is only useful if you included HOG features during training\n",
    "    \n",
    "    # output image\n",
    "    draw_img = np.copy(img)\n",
    "    \n",
    "    # Make a heatmap of zeros\n",
    "    heatmap = np.zeros_like(img[:,:,0])\n",
    "    \n",
    "    # Convert the image pixel value range so that jpg images are same as png images - training was done on png\n",
    "    img = img.astype(np.float32) / 255\n",
    "    \n",
    "    # Using desired start/stop positions we can reduce the image size that needs to be searched\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    \n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    \n",
    "    # Get the desired section of the image based on the start/stop definitions\n",
    "    search_img = img[y_start_stop[0]:y_start_stop[1], x_start_stop[0]:x_start_stop[1], :]\n",
    "    \n",
    "    # Convert it to the desired color space\n",
    "    search_img_color = cs_convert_from_rgb(search_img, color_space = color_space)\n",
    "\n",
    "    # Scale the image if a scale value other than 1 is defined.\n",
    "    # Rather than selecting different window sizes, we scale the image so that window size stays same\n",
    "    # but spatial information corresponding to the window changes due to scaling of the image\n",
    "    if scale != 1:\n",
    "        img_shape = search_img_color.shape\n",
    "        search_img_color = cv2.resize(search_img_color, (np.int(img_shape[1]/scale),\n",
    "                                                         np.int(img_shape[0]/scale)))\n",
    "    \n",
    "    # Let's get each channel as a separate array\n",
    "    ch1 = search_img_color[:,:,0]\n",
    "    ch2 = search_img_color[:,:,1]\n",
    "    ch3 = search_img_color[:,:,2]\n",
    "    \n",
    "    # Now since we will use the raw HOG features and subset depending on the window position\n",
    "    # We need to define the parameters around getting increments\n",
    "    \n",
    "    # Number of blocks in x and y\n",
    "    nx_blocks = (ch1.shape[1] // pix_per_cell) - 1\n",
    "    ny_blocks = (ch1.shape[0] // pix_per_cell) - 1\n",
    "    \n",
    "    # Number of features per block\n",
    "    nfeat_per_block = orient * cell_per_block**2\n",
    "    \n",
    "    # Window size\n",
    "    window_px = 64\n",
    "    \n",
    "    # Number of blocks per window\n",
    "    nblocks_per_window = (window_px // pix_per_cell) - 1\n",
    "    \n",
    "    # Cells per step\n",
    "    # Instead of overlap, define how many cells to step\n",
    "    # with pix_per_cell = 8 and cells_per_step we get 0.75 overlap\n",
    "    cells_per_step = 2\n",
    "    \n",
    "    # Compute the steps in x and y directions\n",
    "    # How many steps we will do across HOG array to extract features\n",
    "    nxsteps = (nx_blocks - nblocks_per_window) // cells_per_step\n",
    "    nysteps = (ny_blocks - nblocks_per_window) // cells_per_step\n",
    "    \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog1 = get_hog_features(ch1, orient = orient, pix_per_cell = pix_per_cell, cell_per_block = cell_per_block, feature_vec = False)\n",
    "    hog2 = get_hog_features(ch2, orient = orient, pix_per_cell = pix_per_cell, cell_per_block = cell_per_block, feature_vec = False)\n",
    "    hog3 = get_hog_features(ch2, orient = orient, pix_per_cell = pix_per_cell, cell_per_block = cell_per_block, feature_vec = False)\n",
    "    \n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb * cells_per_step\n",
    "            xpos = xb * cells_per_step\n",
    "            \n",
    "            # Extract HOG for this patch\n",
    "            hog_feat1 = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel()\n",
    "            hog_feat2 = hog2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel()\n",
    "            hog_feat3 = hog3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel()\n",
    "            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "            \n",
    "            # coordinates of the top left corner\n",
    "            xleft = xpos * pix_per_cell\n",
    "            ytop = ypos * pix_per_cell\n",
    "            \n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(search_img_color[ytop:ytop + window_px, xleft:xleft + window_px], (64, 64)) # we use 64x64 since training data is of this size\n",
    "            \n",
    "            # Get color features\n",
    "            hist_features, _, _, _, _ = color_hist(subimg, nbins = hist_bins)\n",
    "            \n",
    "            # Get spatial features\n",
    "            spatial_features = bin_spatial(subimg, size = spatial_size)\n",
    "            \n",
    "            # Combine the features and scale it with the scaler\n",
    "            test_features = scaler.transform(np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))\n",
    "            # print(test_features.shape)\n",
    "            \n",
    "            # Make prediction\n",
    "            test_prediction = clf.predict(test_features)\n",
    "            \n",
    "            # If the window has a car in it then draw it on the original image\n",
    "            # Also add heat to the heat map\n",
    "            if test_prediction == 1:\n",
    "                # Use the scale\n",
    "                xbox_left = np.int(xleft * scale)\n",
    "                ytop_draw = np.int(ytop * scale)\n",
    "                win_draw = np.int(window_px * scale)\n",
    "                cv2.rectangle(draw_img, (xbox_left + x_start_stop[0], ytop_draw + y_start_stop[0]),\n",
    "                             (xbox_left + win_draw + x_start_stop[0], ytop_draw + win_draw + y_start_stop[0]),\n",
    "                             (0, 0, 255), thickness = 6)\n",
    "                # img_boxes.append(((xbox_left + x_start_stop[0], ytop_draw + y_start_stop[0]),\n",
    "                #                   (xbox_left + win_draw + x_start_stop[0], ytop_draw + win_draw + y_start_stop[0])))\n",
    "                heatmap[ytop_draw + y_start_stop[0]:ytop_draw + win_draw + y_start_stop[0],\n",
    "                        xbox_left + x_start_stop[0]:xbox_left + win_draw + x_start_stop[0]] += 1\n",
    "    \n",
    "    # Return the image and heatmap\n",
    "    return draw_img, heatmap\n",
    "\n",
    "\n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    # Reduces the false positives by removing some low value regions from the heatmap\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    # Iterate through all detected cars and draw boxes around them\n",
    "    \n",
    "    for car_number in range(1, labels[1] +1):\n",
    "        # Find pixels with each car number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        \n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        \n",
    "        # Define the bounding box based on min/max value of non-zero x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        \n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0, 0, 255), 6)\n",
    "    \n",
    "    # Return the image\n",
    "    return img\n",
    "\n",
    "\n",
    "##Line Detection\n",
    "\n",
    "def get_points(folder_name, out_folder, corner_count, force = True):\n",
    "    \"\"\"Points\"\"\"\n",
    "    output_pickle = 'corners_pickle.p'\n",
    "    \n",
    "    if not os.path.exists(output_pickle) or force:\n",
    "        # Make a list of calibration images\n",
    "        # Glob is useful because there is a pattern in the image file names\n",
    "        file_name_pattern = folder_name + '/*.jpg'\n",
    "        images = glob.glob(file_name_pattern)\n",
    "        \n",
    "        # Initialize the arrays to store the corner information\n",
    "        index_array = []  # this is a 3D array with x, y, z grid locations (real world space)\n",
    "        corners_array = []  # this array will store the corner points in image plane\n",
    "        \n",
    "        # Each chess board has 9x6 corners to detect (inside corners)\n",
    "        # prepare indices, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "        # top left corner will be (0,0,0) and bottom right corner (8,6,0)\n",
    "        indices = np.zeros((corner_count[0] * corner_count[1], 3), np.float32)\n",
    "        \n",
    "        # Now we can use numpy's mgrid to populate the content of the indices array\n",
    "        # We will only assign values to the x, y coordinates\n",
    "        # z position will always be zero as images are 2D\n",
    "        indices[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)\n",
    "        \n",
    "        for idx, img_name in enumerate(images):\n",
    "            print(\"Working on calibration image # \", idx + 1)\n",
    "            \n",
    "            # Read image using cv2\n",
    "            img = cv2.imread(img_name)\n",
    "            # Convert the colorspace to grayscale - reading images using cv2 returns BGR\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Find the chessboard corners using cv2's findChessboardCorners function\n",
    "            # This function requires and image and expected number of inside corners in x and y directions\n",
    "            # The last argument to the function is any flag that we might have. At this points we will set it to None\n",
    "            # Last parameter is for any flags - we don't have any\n",
    "            # The function returns whether or not it was successful and the corner locations\n",
    "            ret, corners = cv2.findChessboardCorners(gray, corner_count, None)\n",
    "            \n",
    "            # When we can find the corners, we will add the resulting information to our arrays\n",
    "            if ret:\n",
    "                # indices will not change\n",
    "                index_array.append(indices)\n",
    "                # Add corners for each image that is successfully identified\n",
    "                corners_array.append(corners)\n",
    "                \n",
    "                # Draw and show the corners on each image using cv2's function\n",
    "                cv2.drawChessboardCorners(img, corner_count, corners, ret)\n",
    "                cv2.imwrite(os.path.join(out_folder, str('calibration_points_' + str(idx + 1) + '.jpg')), img)\n",
    "        \n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Pickle the results for later use\n",
    "        corners_pickle = dict()\n",
    "        corners_pickle[\"corners\"] = corners_array\n",
    "        corners_pickle[\"indices\"] = index_array\n",
    "        pickle.dump(corners_pickle, open(output_pickle, \"wb\"))\n",
    "    else:\n",
    "        print('--corners_pickle.p-- file already exist! Use \\'force=True\\' to overwrite')\n",
    "\n",
    "\n",
    "def get_calibration(out_folder, test_folder, test_img, force = True):\n",
    "    \"\"\"Calibration\"\"\"\n",
    "    input_pickle = 'corners_pickle.p'\n",
    "    output_pickle = 'calibration.p'\n",
    "    \n",
    "    if not os.path.exists(output_pickle) or force:\n",
    "        if os.path.exists(input_pickle):\n",
    "            # Read the corner information from the pickle file\n",
    "            corners_pickle = pickle.load(open(input_pickle, 'rb'))\n",
    "            indices = corners_pickle['indices']\n",
    "            corners = corners_pickle['corners']\n",
    "            \n",
    "            # Read the calibration test image\n",
    "            img = cv2.imread(os.path.join(test_folder, test_img))\n",
    "            img_size = (img.shape[1], img.shape[0])\n",
    "            \n",
    "            # Do camera calibration given object points and image points\n",
    "            # mtx is the camera matrix\n",
    "            # dist = distortion coefficients\n",
    "            # rvecs, tvecs = position of the camera in real world with rotation and translation vecs\n",
    "            ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(indices, corners, img_size, None, None)\n",
    "            \n",
    "            # Test calibration on an image - undistort and save\n",
    "            # This is usually called destination image\n",
    "            dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "            cv2.imwrite(os.path.join(out_folder, test_img), dst)\n",
    "            \n",
    "            # Save calibration matrix and distortion coefficients\n",
    "            calibration = dict()\n",
    "            calibration[\"mtx\"] = mtx\n",
    "            calibration[\"dist\"] = dist\n",
    "            pickle.dump(calibration, open(output_pickle, \"wb\"))\n",
    "            \n",
    "            print('--calibration.p-- saved! ')\n",
    "            \n",
    "            f, (ax1, ax2) = plt.subplots(1, 2, figsize = (9, 3))\n",
    "            f.tight_layout()\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title('Original Image', fontsize = 15)\n",
    "            ax2.imshow(dst)\n",
    "            ax2.set_title('Undistorted Image', fontsize = 15)\n",
    "            plt.subplots_adjust(left = 0., right = 1, top = 0.9, bottom = 0.)\n",
    "            img_name = os.path.splitext(test_img)[0]\n",
    "            plt.savefig(os.path.join(out_folder, str(img_name + '_compare')))\n",
    "            plt.close()\n",
    "        else:\n",
    "            sys.exit('--corners_pickle.p-- does not exist! Call `get_points()` function first')\n",
    "    else:\n",
    "        print('--calibration.p-- file already exist! Use \\'force=True\\' to overwrite')\n",
    "\n",
    "\n",
    "def abs_sobel_thresh(img, orient = 'x', sobel_kernel = 3, thresh = (0, 255)):\n",
    "    \"\"\"Function that applies Sobel in x or y with a given kernel size takes the absolute value of\n",
    "    the gradient scales to 8bit and returns a mask after checking values with the threshold range\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    if orient.upper() == 'X':\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = sobel_kernel)\n",
    "    elif orient.upper() == 'Y':\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = sobel_kernel)\n",
    "    else:\n",
    "        sys.exit('Orientation should be a member of (x, y) in abs_sobel_thresh function')\n",
    "    \n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    scaled_sobel = np.uint8(255 * abs_sobel / np.max(abs_sobel))\n",
    "    \n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "\n",
    "def mag_thresh(img, sobel_kernel = 3, thresh = (0, 255)):\n",
    "    \"\"\"Calculates the derivatives in x and y directions with a given kernel size\n",
    "    Uses the resultant magnitude of the derivatives to mask the image\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = sobel_kernel)\n",
    "    sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = sobel_kernel)\n",
    "    \n",
    "    sobel = np.sqrt(np.square(sobel_x) + np.square(sobel_y))\n",
    "    \n",
    "    scale_factor = np.max(sobel) / 255\n",
    "    gradmag = (sobel / scale_factor).astype(np.uint8)\n",
    "    \n",
    "    binary_output = np.zeros_like(gradmag)\n",
    "    binary_output[(gradmag >= thresh[0]) & (gradmag <= thresh[1])] = 1\n",
    "    \n",
    "    return binary_output\n",
    "\n",
    "\n",
    "def dir_threshold(img, sobel_kernel = 3, thresh = (0, np.pi / 2)):\n",
    "    \"\"\"Calculates the derivatives in x and y directions with a given kernel size\n",
    "    Then calculates the direction of the resultant vector formed by the x and y gradients\n",
    "    Uses this direction information and threshold values provided to mask the image\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    abs_sobelx = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = sobel_kernel))\n",
    "    abs_sobely = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = sobel_kernel))\n",
    "    \n",
    "    grad_dir = np.arctan2(abs_sobely, abs_sobelx)\n",
    "    \n",
    "    binary_output = np.zeros_like(grad_dir)\n",
    "    binary_output[(grad_dir >= thresh[0]) & (grad_dir <= thresh[1])] = 1\n",
    "    \n",
    "    return binary_output\n",
    "\n",
    "\n",
    "def hls_select(img, chn, thresh = (0, 255)):\n",
    "    \"\"\"Function that converts to HLS color space\n",
    "    Applies a threshold to the desired channel and returns the mask\"\"\"\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "    \n",
    "    if chn.upper() == 'H':\n",
    "        chn_select = hls[:, :, 0]\n",
    "    elif chn.upper() == \"L\":\n",
    "        chn_select = hls[:, :, 1]\n",
    "    elif chn.upper() == \"S\":\n",
    "        chn_select = hls[:, :, 2]\n",
    "    else:\n",
    "        sys.exit('Select from (H, L, S) as the channel argument for hls_select() function')\n",
    "    \n",
    "    binary_output = np.zeros_like(chn_select)\n",
    "    binary_output[(chn_select >= thresh[0]) & (chn_select <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "\n",
    "def hsv_select(img, chn, thresh = (0, 255)):\n",
    "    \"\"\"Function that converts to HSV color space\n",
    "    Applies a threshold to the desired channel and returns the mask\"\"\"\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    if chn.upper() == 'H':\n",
    "        chn_select = hsv[:, :, 0]\n",
    "    elif chn.upper() == \"S\":\n",
    "        chn_select = hsv[:, :, 1]\n",
    "    elif chn.upper() == \"V\":\n",
    "        chn_select = hsv[:, :, 2]\n",
    "    else:\n",
    "        sys.exit('Select from (H, S, V) as the channel argument for hsv_select() function')\n",
    "    \n",
    "    binary_output = np.zeros_like(chn_select)\n",
    "    binary_output[(chn_select >= thresh[0]) & (chn_select <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "\n",
    "def window_mask(width, height, img_ref, center, level):\n",
    "    \"\"\"Draws boxes\"\"\"\n",
    "    output = np.zeros_like(img_ref)\n",
    "    output[int(img_ref.shape[0] - (level + 1) * height):int(img_ref.shape[0] - level * height), max(0, int(center - width / 2)):min(int(center + width / 2), img_ref.shape[1])] = 1\n",
    "    return output\n",
    "\n",
    "\n",
    "def pipeline(img):\n",
    "    # Get the calibration parameters\n",
    "    calibration_pickle = pickle.load(open('calibration.p', 'rb'))\n",
    "    mtx = calibration_pickle['mtx']\n",
    "    dist = calibration_pickle['dist']\n",
    "    \n",
    "    # Undistort the image based on calibration data\n",
    "    img = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    \n",
    "    # Apply some thresholding methods\n",
    "    gradx = abs_sobel_thresh(img, orient = 'x', sobel_kernel = 3, thresh = (15, 255))\n",
    "    grady = abs_sobel_thresh(img, orient = 'y', sobel_kernel = 3, thresh = (45, 255))\n",
    "     \n",
    "    #mag_binary = mag_thresh(img, sobel_kernel = 3, thresh = (30, 100))\n",
    "    #dir_binary = dir_threshold(img, sobel_kernel = 15, thresh = (0.7, 1.3))\n",
    "    \n",
    "    sthresh = hls_select(img, 's', thresh = (102, 255))\n",
    "    vthresh = hsv_select(img, 'v', thresh = (51, 255))\n",
    "    \n",
    "    # Get the combined binary\n",
    "    combined = np.zeros_like(gradx)\n",
    "    combined[(gradx == 1) & (grady == 1) | ((sthresh == 1) & (vthresh == 1))] = 255\n",
    "    #combined[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1))] = 1\n",
    "    \n",
    "    # Apply perspective Transform\n",
    "    \n",
    "    # First video\n",
    "    box_width = 0.645\n",
    "    mid_width = 0.118\n",
    "    height_pct = 0.645 #468px=,65; 450px=,625, 486px=,675\n",
    "    bottom_trim = 0.934\n",
    "    \n",
    "    src = np.float32([[img.shape[1] * (.5 - mid_width / 2), img.shape[0] * height_pct],\n",
    "                      [img.shape[1] * (.5 + mid_width / 2), img.shape[0] * height_pct],\n",
    "                      [img.shape[1] * (.5 + box_width / 2), img.shape[0] * bottom_trim],\n",
    "                      [img.shape[1] * (.5 - box_width / 2), img.shape[0] * bottom_trim]])\n",
    "    \n",
    "    # offset adjusts the shrinkage of the warped image - larger is shrunken more\n",
    "    offset = img.shape[1] * 0.175\n",
    "    \n",
    "    dst = np.float32([[offset, 0], [img.shape[1] - offset, 0],\n",
    "                      [img.shape[1] - offset, img.shape[0]],\n",
    "                      [offset, img.shape[0]]])\n",
    "    \n",
    "    # Get perspective transformation matrix and its inverse for later use\n",
    "    mat = cv2.getPerspectiveTransform(src, dst)\n",
    "    mat_inv = cv2.getPerspectiveTransform(dst, src)\n",
    "    \n",
    "    # Use the matrix for perspective transform\n",
    "    binary_warped = cv2.warpPerspective(combined, mat, (img.shape[1], img.shape[0]), flags = cv2.INTER_LINEAR)\n",
    "    \n",
    "    # window settings\n",
    "    window_width = 30  # consider a window width\n",
    "    window_height = 120  # Break image into 6 vertical layers since image height is 720\n",
    "    margin = 30  # How much to slide left and right for searching\n",
    "    smoothing_factor = 20  # Use last 20 results for averaging - smooth the data 20\n",
    "    # The lane is about 30 meters long and 3.7 meters wide\n",
    "    y_scale = 30.0 / 720.0  # 30 meter is around 720 pixels\n",
    "    x_scale = 3.7 / 700.0  # 3.7 meters is around 700pixels\n",
    "    \n",
    "    # Setup the overall class to do all the tracking\n",
    "    centroids = Line(window_width = window_width, window_height = window_height, margin = margin, ym = y_scale, xm = x_scale, smooth_factor = smoothing_factor)\n",
    "    \n",
    "    window_centroids = centroids.find_window_centroids(binary_warped)\n",
    "    \n",
    "    # If we found any window centers\n",
    "    if len(window_centroids) > 0:\n",
    "        # Points used to draw all the left and right windows\n",
    "        l_points = np.zeros_like(binary_warped)\n",
    "        r_points = np.zeros_like(binary_warped)\n",
    "        \n",
    "        # Points used to find the left and right lanes\n",
    "        rightx = []\n",
    "        leftx = []\n",
    "        \n",
    "        # Go through each level and draw the windows\n",
    "        for level in range(0, len(window_centroids)):\n",
    "            # add center value found in frame to the list of lane points per left, right\n",
    "            leftx.append(window_centroids[level][0])\n",
    "            rightx.append(window_centroids[level][1])\n",
    "            \n",
    "            # window_mask is a function to draw window areas\n",
    "            l_mask = window_mask(window_width, window_height, binary_warped, window_centroids[level][0], level) #window_width - 35\n",
    "            r_mask = window_mask(window_width, window_height, binary_warped, window_centroids[level][1], level) #window_width - 35\n",
    "            \n",
    "            # Add graphic points from window mask here to total pixels found\n",
    "            l_points[(l_points == 255) | ((l_mask == 1))] = 255\n",
    "            r_points[(r_points == 255) | ((r_mask == 1))] = 255\n",
    "        \n",
    "        # Draw the results\n",
    "        template = np.array(r_points + l_points, np.uint8)  # add both left and right window pixels together\n",
    "        zero_channel = np.zeros_like(template)  # create a zero color channel\n",
    "        template = np.array(cv2.merge((zero_channel, template, zero_channel)), np.uint8)  # make window pixels green\n",
    "        # making the original road pixels 3 color channel\n",
    "        warpage = np.array(cv2.merge((binary_warped, binary_warped, binary_warped)), np.uint8)\n",
    "        # overlay the original road image with window results\n",
    "        output = cv2.addWeighted(warpage, 1.0, template, 0.5, 0.0)\n",
    "        \n",
    "        # fit the lane boundaries to the left, right and center positions found\n",
    "        yvals = range(0, binary_warped.shape[0])\n",
    "        \n",
    "        # box centers - should be 9 components\n",
    "        res_yvals = np.arange(binary_warped.shape[0] - (window_height / 2), 0, -window_height)\n",
    "        \n",
    "        # fit polynomial to left - 2nd order\n",
    "        left_fit = np.polyfit(res_yvals, leftx, 2)\n",
    "        # predict the x value for each y value - continuous with resolution of 1 pixel\n",
    "        left_fitx = left_fit[0] * yvals * yvals + left_fit[1] * yvals + left_fit[2]\n",
    "        left_fitx = np.array(left_fitx, np.int32)\n",
    "        \n",
    "        # fit polynomial to right - 2nd order\n",
    "        right_fit = np.polyfit(res_yvals, rightx, 2)\n",
    "        # predict the x value for each y value - continuous with resolution of 1 pixel\n",
    "        right_fitx = right_fit[0] * yvals * yvals + right_fit[1] * yvals + right_fit[2]\n",
    "        right_fitx = np.array(right_fitx, np.int32)\n",
    "        \n",
    "        # encapsulate lines to give depth\n",
    "        # left lane\n",
    "        left_lane = np.array(list(zip(np.concatenate((left_fitx - (window_width) / 2, left_fitx[::-1] + (window_width) / 2), axis = 0),\n",
    "                                      np.concatenate((yvals, yvals[::-1]), axis = 0))), np.int32) #window_width - 35\n",
    "        # right lane\n",
    "        right_lane = np.array(list(zip(np.concatenate((right_fitx - (window_width) / 2, right_fitx[::-1] + (window_width) / 2), axis = 0),\n",
    "                                       np.concatenate((yvals, yvals[::-1]), axis = 0))), np.int32) #window_width - 35\n",
    "        \n",
    "        # inner lane\n",
    "        inner_lane = np.array(list(zip(np.concatenate((left_fitx + (window_width) / 2, right_fitx[::-1] - (window_width) / 2), axis = 0),\n",
    "                                       np.concatenate((yvals, yvals[::-1]), axis = 0))), np.int32) #window_width - 35\n",
    "        \n",
    "        # lane lines themselves\n",
    "        road = np.zeros_like(img)\n",
    "        cv2.fillPoly(road, [left_lane], color = [255, 0, 0])\n",
    "        cv2.fillPoly(road, [right_lane], color = [0, 0, 255])\n",
    "        cv2.fillPoly(road, [inner_lane], color = [0, 255, 0])\n",
    "        # inverse transform to get back to actual perspective\n",
    "        road_warped = cv2.warpPerspective(road, mat_inv, (img.shape[1], img.shape[0]), flags = cv2.INTER_LINEAR)\n",
    "        \n",
    "        # to get nice outlines\n",
    "        road_bkg = np.zeros_like(img)\n",
    "        cv2.fillPoly(road_bkg, [left_lane], color = [255, 255, 255])\n",
    "        cv2.fillPoly(road_bkg, [right_lane], color = [255, 255, 255])\n",
    "        cv2.fillPoly(road_bkg, [inner_lane], color = [255, 255, 255])\n",
    "        # inverse transform to get back to actual perspective\n",
    "        road_warped_bkg = cv2.warpPerspective(road_bkg, mat_inv, (img.shape[1], img.shape[0]), flags = cv2.INTER_LINEAR)\n",
    "        \n",
    "        # return the transformed lane lines\n",
    "        # first make background black\n",
    "        base = cv2.addWeighted(img, 1.0, road_warped_bkg, -1.0, 0.0)\n",
    "        # then add the lane lines\n",
    "        result = cv2.addWeighted(base, 1.0, road_warped, 0.7, 0.0)\n",
    "        output = result\n",
    "        \n",
    "        # meters per pixel in y direction\n",
    "        xm_ppx, ym_ppx = centroids.get_ppx_values()\n",
    "        \n",
    "        # fit a 2nd order polynomial for the actual x and y coordinates of the left lane\n",
    "        # left lane is more stable\n",
    "        curve_fit_cr = np.polyfit(np.array(res_yvals, np.float32) * ym_ppx, np.array(leftx, np.float32) * xm_ppx, 2)\n",
    "        # using the formula calculate the road curvature\n",
    "        curverad = ((1 + (2 * curve_fit_cr[0] * yvals[-1] * ym_ppx + curve_fit_cr[1]) ** 2) ** 1.5) / np.absolute(2 * curve_fit_cr[0])\n",
    "        \n",
    "        # calculate the offset of the car on the road\n",
    "        # average the x pixel values that are closest to the car to find the road center\n",
    "        road_center = (left_fitx[-1] + right_fitx[-1]) / 2\n",
    "        # find the difference between the road center and the warped image center - convert it to actual meters\n",
    "        center_diff = (road_center - binary_warped.shape[1]/2) * xm_ppx\n",
    "        side_pos = \"left\"\n",
    "        if center_diff <= 0:\n",
    "            # if difference is smaller than zero, warped image center (and hence the car) location\n",
    "            # is to the right of the road\n",
    "            side_pos = \"right\"\n",
    "        \n",
    "        # draw the text showing curvature, offset and speed\n",
    "        # can check the values with\n",
    "        cv2.putText(output, 'Radius of Curvature = ' + str(round(curverad, 3)) + '(m)', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.putText(output, 'Vehicle is = ' + str(abs(round(center_diff, 3))) + 'm ' + side_pos + ' of center', (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    else:\n",
    "        output = img\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def process_image(img):\n",
    "    # Main Proces\n",
    "    x_start_stop = [None, None]\n",
    "    y_start_stop = [400, 672]  # Min and max in y to search in slide_window()\n",
    "    scale = 1.5\n",
    "    #scales = [1.0, 1.5]\n",
    "    #threshold = 0.3\n",
    "    \n",
    "    # Advanced Line Detection\n",
    "    out_img_pipe = pipeline(img = img)\n",
    "    \n",
    "    \n",
    "    #heat_maps = []\n",
    "    #for scale in scales:\n",
    "    out_img, out_ = find_cars(img = img, scale = scale, x_start_stop = x_start_stop,\n",
    "                              y_start_stop = y_start_stop, clf = svc, scaler = x_scaler,\n",
    "                              orient = orient, pix_per_cell = pix_per_cell,\n",
    "                              cell_per_block = cell_per_block, spatial_size = spatial_size,\n",
    "                              hist_bins = hist_bins, color_space = color_space)\n",
    "    \n",
    "    #heat_maps.append(out_)\n",
    "    \n",
    "    labels = label(out_)\n",
    "    draw_img = draw_labeled_bboxes(np.copy(out_img_pipe), labels)\n",
    "    #avg_ = np.divide(np.sum(heat_maps, axis = 0), len(scales))\n",
    "    #final_ = apply_threshold(avg_, threshold)\n",
    "    #labels = label(final_)\n",
    "    #draw_img = draw_labeled_bboxes(np.copy(out_img_pipe), labels)\n",
    "    return draw_img\n",
    "\n",
    "\n",
    "def process_video(input_video_path, out_path, process_func):\n",
    "    # A function to process video images\n",
    "    clip = VideoFileClip(input_video_path)\n",
    "    video_clip = clip.fl_image(process_func)\n",
    "    video_clip.write_videofile(out_path, audio = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Vehicle Images found:  8792\n",
      "Number of Non-Vehicle Images found:  8968\n"
     ]
    }
   ],
   "source": [
    "# Since we will be identifying cars using a classification algorithm, \n",
    "# we need to train the classifier using car and non-car images. \n",
    "# These images are located in two different folders 'vehicles' and 'non-vehicles' with subfolders.\n",
    "# These example images come from a combination of the GTI vehicle image database, \n",
    "# the KITTI vision benchmark suite, and examples extracted from the project video itself.\n",
    "# We can use glob package to get various png images under different subfolders using a pattern \n",
    "\n",
    "car_image_paths = glob.glob('vehicles/*/*.png')\n",
    "notcar_image_paths = glob.glob('non-vehicles/*/*.png')\n",
    "\n",
    "# Now let's see the total number of images we have in each class \n",
    "print('Number of Vehicle Images found: ', len(car_image_paths))\n",
    "print('Number of Non-Vehicle Images found: ', len(notcar_image_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.48 Seconds to compute the features\n",
      "\n",
      "Using: 9 orientations 8 pixels per cell and 2 cells per block 32 histogram bins, and (32, 32) spatial sampling\n",
      "\n",
      "Feature vector length: 8460 \n",
      "\n",
      "5.94 Seconds to train SVC. \n",
      "\n",
      "Test accuracy of SVC =  0.9916 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define feature parameters\n",
    "color_space = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 9\n",
    "pix_per_cell = 8 # size of the features we are looking in the images \n",
    "cell_per_block = 2 # helps with the normalization - lighting, shadows\n",
    "hog_channel = 'ALL'  # can be 0, 1, 2, or 'ALL' \n",
    "spatial_size = (32, 32)  # Spatial binning dimensions \n",
    "hist_bins = 32  # Number of histogram bins\n",
    "spatial_feat = True  # Get spatial features on/off\n",
    "hist_feat = True  # Get color histogram features on/off\n",
    "hog_feat = True  # Get HOG features on/off\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "n_samples = 2000\n",
    "random_idxs = np.random.randint(0, len(car_image_paths), n_samples)\n",
    "\n",
    "test_cars = car_image_paths\n",
    "test_notcars = notcar_image_paths\n",
    "\n",
    "car_features = extract_features(test_cars, color_space = color_space,\n",
    "                                spatial_size = spatial_size, hist_bins = hist_bins,\n",
    "                                orient = orient, pix_per_cell = pix_per_cell,\n",
    "                                cell_per_block = cell_per_block, hog_channel = hog_channel,\n",
    "                                spatial_feat = spatial_feat, hist_feat = hist_feat, hog_feat = hog_feat)\n",
    "\n",
    "notcar_features = extract_features(test_notcars, color_space = color_space,\n",
    "                                   spatial_size = spatial_size, hist_bins = hist_bins,\n",
    "                                   orient = orient, pix_per_cell = pix_per_cell,\n",
    "                                   cell_per_block = cell_per_block, hog_channel = hog_channel,\n",
    "                                   spatial_feat = spatial_feat, hist_feat = hist_feat, hog_feat = hog_feat)\n",
    "\n",
    "print(round(time.time() - t, 2), 'Seconds to compute the features\\n')\n",
    "\n",
    "x = np.vstack((car_features, notcar_features)).astype(np.float64) # Standard Scaler expects float 64\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "# Normalize Data\n",
    "# We have spatial, color histogram and HOG features in the same feature set\n",
    "# It is best to bring them to equal scale to avoid one feature to dominate due to scale differences\n",
    "# Fit a per-column scaler\n",
    "x_scaler = StandardScaler().fit(x)\n",
    "# Use the scaler to transform X \n",
    "scaled_x = x_scaler.transform(x)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_x, y, test_size = 0.2, random_state = rand_state)\n",
    "\n",
    "print('Using:', orient, 'orientations', pix_per_cell, 'pixels per cell and', \n",
    "      cell_per_block, 'cells per block', hist_bins, 'histogram bins, and',\n",
    "      spatial_size, 'spatial sampling\\n')\n",
    "print('Feature vector length:', len(x_train[0]), '\\n')\n",
    "\n",
    "# Use linear SVC\n",
    "svc = LinearSVC()\n",
    "\n",
    "# Check the training time for the SVC\n",
    "t = time.time()\n",
    "\n",
    "svc.fit(x_train, y_train)\n",
    "\n",
    "print(round(time.time() - t, 2), 'Seconds to train SVC. \\n')\n",
    "\n",
    "# Check the score of the SVC\n",
    "svc_score = svc.score(x_test, y_test)\n",
    "\n",
    "print('Test accuracy of SVC = ', round(svc_score, 4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_videos/challenge_test_video.mp4\n",
      "[MoviePy] Writing video output_videos/challenge_test_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|  | 38/39 [00:22<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_videos/challenge_test_video.mp4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_video('test_video.mp4', 'output_videos/challenge_test_video.mp4', process_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "video = io.open('output_videos/challenge_test_video.mp4', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data = '''<video width=\"960\" height=\"540\" alt=\"test_video_car_detection\" controls>\n",
    "               <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "               </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_videos/challenge_project_video.mp4\n",
      "[MoviePy] Writing video output_videos/challenge_project_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1260/1261 [12:20<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_videos/challenge_project_video.mp4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_video('project_video.mp4', 'output_videos/challenge_project_video.mp4', process_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "video = io.open('output_videos/challenge_project_video.mp4', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data = '''<video width=\"960\" height=\"540\" alt=\"project_video_car_detection\" controls>\n",
    "               <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "               </video>'''.format(encoded.decode('ascii')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
